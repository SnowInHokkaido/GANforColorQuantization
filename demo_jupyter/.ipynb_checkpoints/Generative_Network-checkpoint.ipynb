{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import scipy.io\n",
    "import datetime\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.color\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_init_vars(net, out_channels, filter_size, transpose=False):\n",
    "    '''\n",
    "    \n",
    "    According to the previous output, intialize the weight matrix.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    _, rows, cols, in_channels = [i.value for i in net.get_shape()] ### Obtain in_channels\n",
    "    \n",
    "    if not transpose:\n",
    "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
    "    else:\n",
    "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
    "    \n",
    "    # weights shape = [Kernal size, kernal size, output kernal, input kernal]\n",
    "    \n",
    "    xaiver_init_2d = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "    weights_init = tf.Variable(xaiver_init_2d(weights_shape), dtype=tf.float32)\n",
    "    \n",
    "    return weights_init\n",
    "\n",
    "\n",
    "def bias_init_vars(outputfilter):\n",
    "\n",
    "    bias_shape = [outputfilter]\n",
    "    \n",
    "    bias_init = tf.Variable(tf.constant(0.01, shape = bias_shape, dtype=tf.float32))\n",
    "    \n",
    "    return bias_init\n",
    "    \n",
    "    \n",
    "def batch_norm(net, train=True):\n",
    "    '''\n",
    "    \n",
    "    Apply Batch Normalization Function\n",
    "    \n",
    "    BN: Forward norm and then inverse norm.\n",
    "    \n",
    "    '''\n",
    "    batch, rows, cols, channels = [i.value for i in net.get_shape()]### Shape Meaning: [batchsize, height, width, kernels]\n",
    "    var_shape = [channels]\n",
    "    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True) ### Calculate the mean and variance of x.Output: One-dimension\n",
    "    shift = tf.Variable(tf.zeros(var_shape)) ### Inverse Norm\n",
    "    scale = tf.Variable(tf.ones(var_shape)) ### Inverse Norm\n",
    "    epsilon = 1e-3\n",
    "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
    "    return scale * normalized + shift ### Applied Batch Normalization\n",
    "\n",
    "\n",
    "def conv_layer(net, num_filters, filter_size, strides, relu=True):\n",
    "    '''\n",
    "    \n",
    "    Apply convolution operation\n",
    "    \n",
    "    '''\n",
    "    weights_init = conv_init_vars(net, num_filters, filter_size)\n",
    "    bias_init = bias_init_vars(num_filters)\n",
    "    strides_shape = [1, strides, strides, 1]                \n",
    "    net = tf.nn.conv2d(net, weights_init, strides_shape, padding='SAME')\n",
    "    net = tf.nn.bias_add(net, bias_init)\n",
    "    \n",
    "    if relu:\n",
    "        net = lrelu(net)   \n",
    "    \n",
    "    return net        \n",
    "\n",
    "def conv_layer_dila(net, num_filters, filter_size, rate, relu=True):\n",
    "    '''\n",
    "    \n",
    "    Apply dilation convolution operation\n",
    "    \n",
    "    '''\n",
    "    weights_init = conv_init_vars(net, num_filters, filter_size)\n",
    "    #strides_shape = [1, strides, strides, 1]\n",
    "    bias_init = bias_init_vars(num_filters)\n",
    "    net = tf.nn.atrous_conv2d(net, weights_init, rate, 'SAME') # Dialation Convolution\n",
    "    net = tf.nn.bias_add(net, bias_init)    \n",
    "    if relu:\n",
    "        net = lrelu(net)   \n",
    "    \n",
    "    return net   \n",
    "\n",
    "def conv_tranpose_layer(net, num_filters, filter_size, strides, relu = True):\n",
    "    weights_init = conv_init_vars(net, num_filters, filter_size, transpose=True)\n",
    "    bias_init = bias_init_vars(num_filters)\n",
    "    batch_size, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
    "    new_rows, new_cols = int(rows * strides), int(cols * strides)\n",
    "    # new_shape = #tf.pack([tf.shape(net)[0], new_rows, new_cols, num_filters])\n",
    "    new_shape = [batch_size, new_rows, new_cols, num_filters]  \n",
    "    tf_shape = tf.stack(new_shape)   \n",
    "    strides_shape = [1,strides,strides,1]\n",
    "    net = tf.nn.conv2d_transpose(net, weights_init, tf_shape, strides_shape, padding='SAME')\n",
    "    net = tf.nn.bias_add(net, bias_init)\n",
    "    if relu:\n",
    "        net = lrelu(net)   \n",
    "    return net\n",
    "\n",
    "def lrelu(net, alpha = 0.1):\n",
    "    return tf.nn.relu(net) - alpha * tf.nn.relu(-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    with tf.name_scope('GN_layer6'):    \\n        conv6_1_relu = conv_tranpose_layer(conv5_3norm, 256, 3, 2)\\n        conv6_2_relu = conv_layer_dila(conv6_1_relu, 256, 3, 1, relu=True)\\n        conv6_3_relu = conv_layer_dila(conv6_2_relu, 256, 3, 1, relu=True)\\n        conv6_3norm = batch_norm(conv6_3_relu, train=True)   \\n        \\n    with tf.name_scope('GN_layer7'):    \\n        conv7_1_relu = conv_tranpose_layer(conv6_3norm, 256, 3, 2)\\n        conv7_2_relu = conv_layer(conv7_1_relu, 256, 3, 1, relu=True)\\n        conv7_3_relu = conv_layer(conv7_2_relu, 256, 3, 1, relu=True)\\n        conv7_3norm = batch_norm(conv7_3_relu, train=True)   \\n\\n    with tf.name_scope('GN_layer8'):    \\n        conv8_1_relu = conv_tranpose_layer(conv7_3norm, 256, 3, 2)\\n        conv8_2_relu = conv_layer(conv8_1_relu, 256, 3, 1, relu=True)\\n        conv8_3_relu = conv_layer(conv8_2_relu, 256, 3, 1, relu=True) ### Output batch x 256 x 256 x 256\\n\\n\\n    with tf.name_scope('mapping'):\\n        mapping_filter = tf.Variable(np.transpose(rgb_palette, [1,0]), dtype=tf.float32)\\n        mapping_filter = tf.reshape(mapping_filter, [1, 1, 256, 3])\\n        cq_image = tf.nn.conv2d(prob_distribution, mapping_filter, strides=[1,1,1,1], padding='SAME')\\n\\n        cq_image  = tf.clip_by_value(cq_image , 0., 255., name=None)\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def net(image, orig_palette):  ### 输入的图像不用normalization [batch, height = 128, width = 128, channel = 1]\n",
    "    with tf.name_scope('GN_layer1'):\n",
    "        normalization = batch_norm(image, train=True)\n",
    "        conv1_1_relu = conv_layer(normalization, 64, 3, 1, relu=True) #[batch, height = 128, width = 128, channel = 64]\n",
    "        conv1_2_relu = conv_layer(conv1_1_relu, 64, 3, 1, relu=True)\n",
    "        conv1_2norm = batch_norm(conv1_2_relu, train=True)\n",
    "    \n",
    "    with tf.name_scope('GN_layer2'):\n",
    "        conv2_1_relu = conv_layer(conv1_2norm, 128, 3, 1, relu=True)\n",
    "        conv2_2_relu = conv_layer(conv1_1_relu, 128, 3, 2, relu=True) \n",
    "        conv2_2norm = batch_norm(conv2_2_relu, train=True)\n",
    "\n",
    "    with tf.name_scope('GN_layer3'):\n",
    "        conv3_1_relu = conv_layer(conv2_2norm, 256, 3, 1, relu=True)\n",
    "        conv3_2_relu = conv_layer(conv3_1_relu, 256, 3, 1, relu=True)\n",
    "        conv3_3_relu = conv_layer(conv3_2_relu, 256, 3, 2, relu=True)\n",
    "        conv3_3norm = batch_norm(conv3_3_relu, train=True)\n",
    "    '''\n",
    "    conv4_1 (Stride:1,pad:1 dilation: 1)> relu4_1 > conv4_2(same) > relu4_2 > conv4_3(same) > relu4_3 > conv4_3_norm\n",
    "    tf.nn.atrous_conv2d(net, weights_init, rate, 'SAME')\n",
    "    conv_layer_dila(net, num_filters, filter_size, rate, relu=True)\n",
    "    '''\n",
    "    with tf.name_scope('GN_layer4'):\n",
    "        conv4_1_relu = conv_layer(conv3_3norm, 512, 3, 2, relu=True)\n",
    "        conv4_2_relu = conv_layer_dila(conv4_1_relu, 512, 3, 1, relu=True)\n",
    "        conv4_3_relu = conv_layer_dila(conv4_2_relu, 512, 3, 1, relu=True)\n",
    "        conv4_3norm = batch_norm(conv4_3_relu, train=True)\n",
    "        \n",
    "    with tf.name_scope('GN_layer5'):\n",
    "        conv5_1_relu = conv_layer(conv4_3norm, 512, 3, 1, relu=True)\n",
    "        conv5_2_relu = conv_layer_dila(conv5_1_relu, 512, 3, 1, relu=True)\n",
    "        conv5_3_relu = conv_layer_dila(conv5_2_relu, 512, 3, 1, relu=True)\n",
    "        conv5_3norm = batch_norm(conv5_3_relu, train=True)\n",
    "        \n",
    "    with tf.name_scope('GN_layer6'):    \n",
    "        conv6_1_relu = conv_layer(conv5_3norm, 512, 3, 1)\n",
    "        conv6_2_relu = conv_layer_dila(conv6_1_relu, 512, 3, 1, relu=True)\n",
    "        conv6_3_relu = conv_layer_dila(conv6_2_relu, 512, 3, 1, relu=True)\n",
    "        conv6_3norm = batch_norm(conv6_3_relu, train=True)   \n",
    "        \n",
    "    with tf.name_scope('GN_layer7'):    \n",
    "        conv7_1_relu = conv_layer(conv6_3norm, 512, 3, 1)\n",
    "        conv7_2_relu = conv_layer_dila(conv7_1_relu, 512, 3, 1, relu=True)\n",
    "        conv7_3_relu = conv_layer_dila(conv7_2_relu, 512, 3, 1, relu=True)\n",
    "        conv7_3norm = batch_norm(conv7_3_relu, train=True)   \n",
    "\n",
    "    with tf.name_scope('GN_layer8'):    \n",
    "        conv8_1_relu = conv_tranpose_layer(conv7_3norm, 256, 3, 2, relu = False)\n",
    "    \n",
    "    with tf.name_scope('GN_Prob'):\n",
    "        prob_distribution = tf.nn.softmax(conv8_1_relu) \n",
    "    \n",
    "    with tf.name_scope('mapping'):\n",
    "        mapping_filter = tf.constant(np.transpose(orig_palette, [1,0]), dtype=tf.float32)\n",
    "        mapping_filter = tf.reshape(mapping_filter, [1, 1, 256, 2])\n",
    "        cq_image = tf.nn.conv2d(prob_distribution, mapping_filter, strides=[1,1,1,1], padding='SAME')\n",
    "        \n",
    "    cq_image = tf.image.resize_images(cq_image, size = (128, 128), method = 0)\n",
    "    \n",
    "    \n",
    "    yuv_image = tf.concat((image, cq_image), axis = 3)\n",
    "\n",
    "    return yuv_image\n",
    "'''\n",
    "    with tf.name_scope('GN_layer6'):    \n",
    "        conv6_1_relu = conv_tranpose_layer(conv5_3norm, 256, 3, 2)\n",
    "        conv6_2_relu = conv_layer_dila(conv6_1_relu, 256, 3, 1, relu=True)\n",
    "        conv6_3_relu = conv_layer_dila(conv6_2_relu, 256, 3, 1, relu=True)\n",
    "        conv6_3norm = batch_norm(conv6_3_relu, train=True)   \n",
    "        \n",
    "    with tf.name_scope('GN_layer7'):    \n",
    "        conv7_1_relu = conv_tranpose_layer(conv6_3norm, 256, 3, 2)\n",
    "        conv7_2_relu = conv_layer(conv7_1_relu, 256, 3, 1, relu=True)\n",
    "        conv7_3_relu = conv_layer(conv7_2_relu, 256, 3, 1, relu=True)\n",
    "        conv7_3norm = batch_norm(conv7_3_relu, train=True)   \n",
    "\n",
    "    with tf.name_scope('GN_layer8'):    \n",
    "        conv8_1_relu = conv_tranpose_layer(conv7_3norm, 256, 3, 2)\n",
    "        conv8_2_relu = conv_layer(conv8_1_relu, 256, 3, 1, relu=True)\n",
    "        conv8_3_relu = conv_layer(conv8_2_relu, 256, 3, 1, relu=True) ### Output batch x 256 x 256 x 256\n",
    "\n",
    "\n",
    "    with tf.name_scope('mapping'):\n",
    "        mapping_filter = tf.Variable(np.transpose(rgb_palette, [1,0]), dtype=tf.float32)\n",
    "        mapping_filter = tf.reshape(mapping_filter, [1, 1, 256, 3])\n",
    "        cq_image = tf.nn.conv2d(prob_distribution, mapping_filter, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "        cq_image  = tf.clip_by_value(cq_image , 0., 255., name=None)\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img(img_path):\n",
    "    img = scipy.misc.imread(img_path, mode = 'RGB')\n",
    "    return img\n",
    "\n",
    "def rgb2lab(image):\n",
    "    '''\n",
    "    L range: 0 ~ 100\n",
    "    a range: -128 ~ 127\n",
    "    b range: -128 ~ 127\n",
    "    \n",
    "    '''\n",
    "    lab_color = skimage.color.rgb2lab(image)\n",
    "    return lab_color\n",
    "\n",
    "def lab2rgb(image):\n",
    "    rgb_color = skimage.color.lab2rgb(image)\n",
    "    return rgb_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Data prepocessing\n",
    "\n",
    "'''\n",
    "palette = np.load('hs_palette.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'HSVToRGB:0' shape=(3, 128, 128, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = tf.constant(1,dtype = tf.float32, shape = [3,128,128,1])\n",
    "\n",
    "image = net(input_img, palette)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
