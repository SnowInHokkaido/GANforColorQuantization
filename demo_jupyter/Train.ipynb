{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from argparse import ArgumentParser\n",
    "import Generative_Network\n",
    "import Discriminative_Network\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTrick: 尽量使用LeakyReLU作为激活函数\\nratio: 10 - 20\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default arguments\n",
    "ORIGINALPALETTE = 'forest_palette.npy'\n",
    "TRAININGDATAPATH = 'train_data.npy'\n",
    "VGG_PATH = 'imagenet-vgg-verydeep-19.mat'\n",
    "BATCHSIZE = 4\n",
    "ITERATIONS = 100000\n",
    "TRAININGRATIO = 10\n",
    "LEARNINGRATE = 1e-4\n",
    "\n",
    "'''\n",
    "\n",
    "Trick: 尽量使用LeakyReLU作为激活函数\n",
    "ratio: 10 - 20\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_parser():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--palettepath',\n",
    "            dest='palettepath', help='load the original color palette',\n",
    "            metavar='ORIGINALPALETTE', default=ORIGINALPALETTE)\n",
    "    parser.add_argument('--trainingdatapath',\n",
    "            dest='trainingdatapath', help='training data in npy format',\n",
    "            metavar='TRAININGDATAPATH', default=TRAININGDATAPATH)\n",
    "    parser.add_argument('--vggpath',\n",
    "            dest='vggpath', help='load pre-trained VGG19 model',\n",
    "            metavar='VGG_PATH', default=VGG_PATH)\n",
    "    parser.add_argument('--iterations', type=int,\n",
    "            dest='iterations', help='iterations (default %(default)s)',\n",
    "            metavar='ITERATIONS', default=ITERATIONS)\n",
    "    parser.add_argument('--batchsize', type=int,\n",
    "            dest='batchsize', help='define the size of batch in training',\n",
    "            metavar='BATCHSIZE', default=BATCHSIZE)\n",
    "    parser.add_argument('--trainingratio', type=int,\n",
    "            dest='trainingratio', help='define the iteration ratio between DN and GN',\n",
    "            metavar='TRAININGRATIO', default=TRAININGRATIO)\n",
    "    parser.add_argument('--learningrate', type=int,\n",
    "            dest='learningrate', help='define the learning rate of Adamoptimizaer',\n",
    "            metavar='LEARNINGRATE', default=LEARNINGRATE) \n",
    "    \n",
    "    return parser\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_network(image, rgb_palette, reuse=False,):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    fake_image = Generative_Network.net(image, rgb_palette)\n",
    "    return fake_image\n",
    "\n",
    "def d_network(image, weights, reuse = False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()    \n",
    "    prob, logits = Discriminative_Network.discriminator(image, weights)   \n",
    "    return prob, logits\n",
    "\n",
    "def next_batch(num, data):\n",
    "    '''\n",
    "    Return a total of `num` random samples\n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = np.array([data[i] for i in idx])\n",
    "    return data_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = build_parser()\n",
    "    options = parser.parse_args()\n",
    "    \n",
    "    #Palette loading\n",
    "    palettepath = options.palettepath\n",
    "    rgb_palette = np.load(palettepath)\n",
    "    \n",
    "    #Training Data Preparation\n",
    "    training_path = options.trainingdatapath\n",
    "    training_data = np.load(training_path)\n",
    "    imagelist = []\n",
    "    for i in range(295):\n",
    "        imagelist.append(np.squeeze(training_data[i,:,:,:]))\n",
    "    \n",
    "    #VGG Model Loading\n",
    "    net_path = options.vggpath\n",
    "    weights, mean_pixel = Discriminative_Network.load_net(net_path)\n",
    "    \n",
    "    learningrate = options.learningrate\n",
    "    batch_size = options.batchsize\n",
    "    iterations = options.iterations\n",
    "    trainingratio = options.trainingratio\n",
    "    gn_iterations = iterations / trainingratio\n",
    "    \n",
    "    print('Data loading completed')\n",
    "\n",
    "    g = tf.Graph()\n",
    "\n",
    "    with g.as_default():\n",
    "\n",
    "        X = tf.placeholder(tf.float32, [batch_size, 128, 128, 3])\n",
    "\n",
    "        with tf.name_scope('generating'):\n",
    "            G_sample = g_network(X, rgb_palette)\n",
    "\n",
    "        with tf.name_scope('discriminating'):\n",
    "            mean_tensor = tf.cast(np.reshape(mean_pixel, [1,1,1,3]), tf.float32)\n",
    "            X_ = X - mean_tensor\n",
    "            G_sample_ = G_sample - mean_tensor       \n",
    "            D_real, D_logit_real = d_network(X_, weights) \n",
    "            D_fake, D_logit_fake = d_network(G_sample_, weights)\n",
    "\n",
    "        with tf.name_scope('D_loss'):\n",
    "            D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "            D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "            D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "        with tf.name_scope('G_loss'):\n",
    "            G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "        tvar = tf.trainable_variables()\n",
    "        dvar = [var for var in tvar if 'discriminator' in var.name] # Find variable in DN\n",
    "        gvar = [var for var in tvar if 'generator' in var.name] # Find variable in GB\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            d_train = tf.train.AdamOptimizer(learningrate).minimize(D_loss, var_list=dvar)\n",
    "            g_train = tf.train.AdamOptimizer(learningrate).minimize(G_loss, var_list=gvar)\n",
    "\n",
    "        print('Graph establised')\n",
    "\n",
    "    with tf.Session(graph = g) as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(10000): # Train ratio: DN/GN = 100/1\n",
    "            print('Training Step:' + str(i+1))\n",
    "            \n",
    "            batch_img = next_batch(batch_size, imagelist)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                samples = sess.run(G_sample, feed_dict={X: batch_img})\n",
    "                if not os.path.isdir('tmp_output'):\n",
    "                    os.mkdir('tmp_output')\n",
    "                    \n",
    "                for index in range(batch_size):\n",
    "                    tmp_name = str(index)\n",
    "                    scipy.misc.imsave('tmp_output/iteration_' + str(i) +'_' + tmp_name +'.jpg', np.squeeze(samples[index,:,:,:]))\n",
    "                \n",
    "\n",
    "            _, D_loss_curr = sess.run([d_train, D_loss], feed_dict={X: batch_img})\n",
    "            _, G_loss_curr = sess.run([g_train, G_loss], feed_dict={X: batch_img})\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(D_loss_curr, G_loss_curr)      \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed\n",
      "Graph establised\n"
     ]
    }
   ],
   "source": [
    "palettepath = ORIGINALPALETTE\n",
    "rgb_palette = np.load(palettepath)\n",
    "\n",
    "#Training Data Preparation\n",
    "training_path = TRAININGDATAPATH\n",
    "training_data = np.load(training_path)\n",
    "imagelist = []\n",
    "for i in range(295):\n",
    "    imagelist.append(np.squeeze(training_data[i,:,:,:]))\n",
    "\n",
    "#VGG Model Loading\n",
    "net_path = VGG_PATH\n",
    "weights, mean_pixel = Discriminative_Network.load_net(net_path)\n",
    "\n",
    "learningrate = LEARNINGRATE\n",
    "batch_size = BATCHSIZE\n",
    "iterations = ITERATIONS\n",
    "trainingratio = TRAININGRATIO\n",
    "gn_iterations = iterations / trainingratio\n",
    "\n",
    "print('Data loading completed')\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [batch_size, 128, 128, 3])\n",
    "    \n",
    "    with tf.name_scope('generating'):\n",
    "        G_sample = g_network(X, rgb_palette)\n",
    "    \n",
    "    with tf.name_scope('discriminating'):\n",
    "        mean_tensor = tf.cast(np.reshape(mean_pixel, [1,1,1,3]), tf.float32)\n",
    "        X_ = X - mean_tensor\n",
    "        G_sample_ = G_sample - mean_tensor       \n",
    "        D_real, D_logit_real = d_network(X_, weights)\n",
    "        D_fake, D_logit_fake = d_network(G_sample_, weights)\n",
    "\n",
    "    with tf.name_scope('D_loss'):\n",
    "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    with tf.name_scope('G_loss'):\n",
    "        G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "    tvar = tf.trainable_variables()\n",
    "    dvar = [var for var in tvar if 'discriminator' in var.name] # Find variable in DN\n",
    "    gvar = [var for var in tvar if 'generator' in var.name] # Find variable in GB\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        d_train = tf.train.AdamOptimizer(learningrate).minimize(D_loss, var_list=dvar)\n",
    "        g_train = tf.train.AdamOptimizer(learningrate).minimize(G_loss, var_list=gvar)\n",
    "        \n",
    "    print('Graph establised')\n",
    "    \n",
    "with tf.Session(graph = g) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    batch_img = next_batch(batch_size, imagelist)\n",
    "    samples = sess.run(G_sample, feed_dict={X: batch_img})\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for i in range(10000): # Train ratio: DN/GN = 100/1\n",
    "        print(i)\n",
    "        batch_img = next_batch(batch_size, imagelist)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            samples = sess.run(G_sample, feed_dict={X: batch_img})\n",
    "    \n",
    "        _, D_loss_curr = sess.run([d_train, D_loss], feed_dict={X: batch_img})\n",
    "        _, G_loss_curr = sess.run([g_train, G_loss], feed_dict={X: batch_img})\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(D_loss_curr, G_loss_curr)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.isdir('tmp_output'):\n",
    "    os.mkdir('tmp_output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    tmp_name = str(i)\n",
    "    scipy.misc.imsave('tmp_output/step1_' + tmp_name +'.jpg', np.squeeze(samples[i,:,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
